# -*- coding: utf-8 -*-
"""RL_Riesgo_Cardiovascular

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q_RG6D20lS01mXOgTTrdR5nld6vjqwPz

Este proyecto tiene como objetivo desarrollar y evaluar un modelo de Machine Learning capaz de predecir el riesgo de enfermedad cardiovascular a 10 años utilizando el Framingham Heart Study Dataset. Se busca hacer enfasis en la sensibilidad para minimizar falsos negativos, lo cual es clave en un contexto de medicina preventiva.

# 1. Cargar las librerias inciales para visualizar los datos y montar la ruta al csv
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
#Montar librerias esenciales

from google.colab import drive
drive.mount('/content/drive')#Se monta el drive primero

ruta=("/content/drive/MyDrive/Maestria IA en Salud/Modulo 2/heart_disease.csv")
#Define la ruta donde se encuentra el archivo csv dentro del drive

df=pd.read_csv(ruta) #Lee el archivo csv usando pandas

print("Primeras filas del dataset:")
display(df.head()) #muestra 5 primeros items, ideal para ver columnas y tipo de datos

print("\nResumen de columnas, tipos de datos y valores nulos:")
df.info()#Informacion sobre la estructura de los datos

print("\nEstadísticas descriptivas:")
df.describe() #Visualizacion matematica de los datos

print("Valores nulos por columna:")
df.isnull().sum() #Cuantos nulos hay por columna

"""Una vez visualizados los datos me quedo con lo siguiente:


*   16 columnas y 4238 entradas
*   No hay datos en str pero si existen nulos que hay que corregir
*   La categoria objetivo es TenYearCHD, las demas son caracteristicas
*   Existen nulos en: education, cigsPerDay, BPMeds, totChol, BMI, heartRate, glucose. La mas afectada es glucose
*   La edad maxima es de 70 y la minima de 32
*   Hombre = 1 y Mujer = 0
*   No todos los pacientes fuman asi que CigsPerDay = 0 o NA

# 2. Visualizar la distribucion de los datos (variable objetivo)
"""

#Este bloque de codigo solo para visualizar variable BINARIA
"""Si se coloca otra variable que no sea binaria
el chart no va a funcionar correctamente"""

sns.countplot(x='TenYearCHD', data=df, palette= "Set2", hue="TenYearCHD")
plt.title('Distribución de TenYearCHD')
plt.xlabel('Riesgo (0 = Bajo, 1 = Alto)')
plt.ylabel('Cantidad de registros')
plt.show()

# Porcentaje de clases
print("Proporción de clases (%):")
print((df["TenYearCHD"].value_counts(normalize=True)*100).round(2))

"""### **LOS DATOS ESTAN DESBALANCEADOS**
### En medicina es normal tener una variable objetivo con este tipo de distribucion, es mas frecuente tener pacientes sanos cuando se estudian datos para prevencion o cribado. Esto se debe tener en cuenta al en las fases finales porque el algoritmo puede resultar siendo muy bueno para predecir sanos y poco fiable prediciendo enfermos y aun asi tener un accuracy elevado.

# 2.1 Visualizar variables caracteristicas (solo numericas)
"""

def ver_distribucion(df, col):
    #Muestra histograma y boxplot de una variable numérica continua
    fig, ax = plt.subplots(1, 2, figsize=(10,4))
    sns.histplot(df[col], kde=True, ax=ax[0], color="skyblue")
    ax[0].set_title(f"Histograma de {col}")
    sns.boxplot(x=df[col], ax=ax[1], color="lightgreen")
    ax[1].set_title(f"Boxplot de {col}")
    plt.show()

# Solo sirve para comparar variables numericas, no binarias
ver_distribucion(df, "age")
#Se puede reemplazar con cualquiera de las otras variables
ver_distribucion(df, "sysBP")
#Se puede reemplazar con cualquiera de las otras variables
ver_distribucion(df, "diaBP")
#Se puede reemplazar con cualquiera de las otras variables
ver_distribucion(df, "cigsPerDay")
#Se puede reemplazar con cualquiera de las otras variables
ver_distribucion(df, "totChol")

sns.countplot(x='currentSmoker', data=df, palette= "Set2", hue="currentSmoker")
plt.title('Fuma?')
plt.xlabel('No = 0 - Si = 1')
plt.ylabel('Cantidad de registros')
plt.show()

# Porcentaje de clases
print("Proporción de clases (%):")
print((df["currentSmoker"].value_counts(normalize=True)*100).round(2))

# Dataframe de solo fumadores totales
smokers_df = df[df['currentSmoker'] == 1]

# Histograma para fumadores y cigarrillos x dia
plt.figure(figsize=(10, 6))
sns.histplot(data=smokers_df, x='cigsPerDay', kde=True, bins=30, color="skyblue")
plt.title('Distribución de cigsPerDay para currentSmoker = 1')
plt.xlabel('Cigarros por día')
plt.ylabel('Numero de pacientes')
plt.show()

print("\nEste histograma no incluye los valores de cigsPerDay = 0 que aportan las columnas currentSmoker = 0")

# Cuantos hombres fuman
hombre_fumador = df[(df['currentSmoker'] == 1) & (df['male'] == 1)]

#
conteo_hombre_fumador = len(hombre_fumador)

print(f"Numero de hombres que fuman: {conteo_hombre_fumador}")

# Cuantas mujeres fuman
mujer_fumador = df[(df['currentSmoker'] == 1) & (df['male'] == 0)]

#
conteo_mujer_fumador = len(mujer_fumador)

print(f"Numero de mujeres que fuman: {conteo_mujer_fumador}")

#Total de fumadores ambos sexos
total_fumadores = conteo_hombre_fumador + conteo_mujer_fumador

print(f"Numero total de fumadores: {total_fumadores}")

#Total de no fumadores ambos sexos
total_no_fumadores = len(df) - total_fumadores

print(f"Numero total de no fumadores: {total_no_fumadores}")

"""Este paso unicamente lo hice para asegurarme de que currentSmoker = 0
corresponde a no fumador y que currentSmoker = 1 es fumador
"""

"""Que se puede observar?

1.   Casi la mitad de los pacientes fuman
2.   La media del totChol esta por encima de los valores normales
1.   La edad media es 49.5 años
2.   La mayoria de fumadores consumen 20 cigarrillos por dia
1.   Mas fumadores que fumadoras

# 2.2 Visualizacion scatterplot
"""

fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 10))
axes = axes.flatten()

# Definir los pares de variables
pares_variables = [
    ('BMI', 'sysBP'),
    ('glucose', 'age'),
    ('glucose', 'BMI'),
    ('age', 'totChol'),
    ('age', 'diaBP'),
    ('age', 'sysBP')
]

# Visualizar scatterplots para entender relaciones y ver outliers mas facilmente
for i, (x_var, y_var) in enumerate(pares_variables):
    sns.scatterplot(data=df, x=x_var, y=y_var, alpha=0.6, ax=axes[i])
    axes[i].set_title(f'Relationship between {x_var} and {y_var}')
    axes[i].set_xlabel(x_var)
    axes[i].set_ylabel(y_var)

plt.tight_layout()
plt.show()

"""# 3. Crear la matriz de correlacion"""

corr_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(14, 12))
sns.heatmap(corr_matrix, cmap="coolwarm", annot=True, linewidths=0.5)
plt.title("Correlacion entre variables")
plt.show()

"""Relaciones fuertes


1.   cigsPerDay y currentSmoker
2.   glucose y diabetes
1.   sysBP y diaBP
2.   diaBP y prevalentHyp

Relaciones no tan fuertes


1.   male con currentSmoker y cigsPerDay
2.   BPMeds con age
1.   age con -> BMI, TenYearCHD, glucose, diaBP, sysBP, totChol,diabetes, prevalentHyp, BPMeds

## Education no sirve para nada, seguramente lo quitare del DataFrame

# Imputacion de datos y normalizacion
"""

"""Toda esta parte del codigo se usa para elmininar los nulos y escalar los datos"""

from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

"""Para evitar data leakage es recomendable primero hacer el train test split
y ajustar (fit) el imputer y el scaler solo en train"""

columna_objetivo = "TenYearCHD"
columna_eliminar = ["education"]

df = df.drop(columns=[c for c in columna_eliminar if c in df.columns])

# Split de datos en grupos train test
X = df.drop(columns=[columna_objetivo]).copy()
y = df[columna_objetivo].copy()

# Imputación con mediana
"""Decidi imputar los datos usando la mediana porque el dataset contiene outliers
que van a tener menos impacto en los resultados ya que este metodo no se deja afectar
por valores extremos y da una representacion centralizada de los valores.
La media tambien es una opcion, rellena nulos
con valores globales pero es util cuando los datos son mas simetricos, este dataset
no es tan simetrico asi que se veria alterado el resultado"""

imputer = SimpleImputer(strategy="median")
scaler = StandardScaler()

"""No se hace imputacion sobre la columna objetivo porque en teoria ya debe estar
completada y no tener nulos. Si tuviera uno que otro nulo seria bueno eliminar
las filas enteras"""

X_imp = imputer.fit_transform(X) #Imputar sobre todo menos objetivo
X_scaled = scaler.fit_transform(X_imp)#Escalar los datos, de nuevo, todo menos objetivo

X_scaled = pd.DataFrame(X_scaled, columns=X.columns)#DataFrame imputado y normalizado

"""### CONFIRMO QUE NO QUEDAN NULOS Y HAN SIDO ESCALADOS LOS DATOS
Seleccione StandardScaler porque transforma los datos para que tengan una media de 0 y una desviación estándar de 1. Es una técnica ideal para la mayoría de los algoritmos de Machine Learning que asumen que las características están centradas en cero y tienen la misma varianza, o que son sensibles a la escala de las características.

# 4. Seleccion de caracteristicas
"""

"""El metodo PCA es el ideal en este caso porque busca representar la mayor
varianza con la menor cantidad de caracteristicas. ICA busca representar las
señales individuales"""

"""El metodo KBest selecciona las mejores caracteristicas pero no hace parte del
temario asi que aplico PCA"""

# Seleccion de caracteristicas
pca_full = PCA().fit(X_scaled)

# Varianza explicada
varianza = pca_full.explained_variance_ratio_
varianza_explicada = np.cumsum(varianza)

plt.plot(range(1, len(varianza_explicada)+1), varianza_explicada, marker="o")
plt.axhline(0.95, linestyle="--", color="r")
plt.xlabel("Número de componentes")
plt.ylabel("Varianza explicada acumulada")
plt.title("Selección de características con PCA")
plt.show()

# Elegir número de componentes (ej: 95% de varianza)
n_components = int(np.searchsorted(varianza_explicada, 0.95) + 1)

# Aplicar PCA definitivo
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X_scaled)

# Crear submuestra depurada (caracteristicas reducidas y objetivo)
df_pca = pd.DataFrame(X_pca, columns=[f"PC{i}" for i in range(1, n_components+1)])
df_pca[columna_objetivo] = y.values

#ver como queda la estructura del Dataframe despues de PCA
print(f"Forma del Dataframe despues de PCA: {X_pca.shape}")

"""###Ajustando el umbral de varianza a 95% vemos que con 12 de las 16 columnas se puede explicar el 95% de la varianza en el Dataset, con este ajuste evitamos que el modelo consuma mas recursos innecesarios, se apoye en datos que no aportan, que se sobreajuste y tambien conseguimos que se entrene mas rapido. Esto ultimo es importante cuando se trabaja con datos mas amplios

# 5. Division Train/Val
"""

X_train, X_val, y_train, y_val = train_test_split(
    df_pca.drop(columns=[columna_objetivo]),
    df_pca[columna_objetivo],
    test_size=0.2,
    stratify=df_pca[columna_objetivo],
    random_state=42
)
print("Estructura entrenamiento:", X_train.shape, " Estructura Val:", X_val.shape)

"""# 6. Entrenar Modelo"""

from sklearn.linear_model import LogisticRegression

"""Se aplicó la opción class_weight='balanced' para compensar el desbalance de clases.
Esto aumentó significativamente la sensibilidad (recall) de la clase positiva,
lo que mejora la capacidad del modelo para identificar correctamente a los pacientes con riesgo,
a costa de un incremento en los falsos positivos. Este cambio es deseable en contextos de salud preventiva,
donde es preferible derivar a más pacientes a evaluación adicional que no detectar un caso real."""

"""Este metodo penaliza mas los errores en la clase minoritaria, y ajusta los
pesos segun el total sin alterar la cantidad de datos de la clase mayoritaria"""

modelo = LogisticRegression(max_iter=1000,class_weight='balanced')
modelo.fit(X_train, y_train)

"""# 7. Comparativa de modelos"""

# Comparacion del rendiemiento de diferentes modelos con PCA aplicado

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve
)

UMBRAL = 0.30

# Modelos a comparar, todos balanceados y usando el mismo umbral de 0.30
modelos = {
    "LogisticRegression": LogisticRegression(max_iter=1000, class_weight="balanced", random_state=42),
    "RandomForest": RandomForestClassifier(n_estimators=300, class_weight="balanced", random_state=42),
    "GradientBoosting": GradientBoostingClassifier(random_state=42),
    "SVM (linear)": SVC(kernel="linear", probability=True, class_weight="balanced", random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}

# Entrenamiento y evaluacion de cada modelo con el mismo umbral
res_list = []
roc_curves = []

for nombre, clf in modelos.items():
    clf.fit(X_train, y_train)
    y_prob = clf.predict_proba(X_val)[:, 1]
    y_pred = (y_prob >= UMBRAL).astype(int)

    res_list.append({
        "Modelo": nombre,
        "Accuracy": accuracy_score(y_val, y_pred),
        "Precision": precision_score(y_val, y_pred, zero_division=0),
        "Recall": recall_score(y_val, y_pred, zero_division=0),
        "F1": f1_score(y_val, y_pred, zero_division=0),
        "AUC": roc_auc_score(y_val, y_prob),
        "Positivos predichos": int(y_pred.sum())
    })

    fpr, tpr, _ = roc_curve(y_val, y_prob)
    auc_val = roc_auc_score(y_val, y_prob)
    roc_curves.append((nombre, fpr, tpr, auc_val))

# Tabla comparativa
df_algos = pd.DataFrame(res_list).sort_values(by=["Recall","AUC"], ascending=[False, False]).reset_index(drop=True)
print("Comparativa de algoritmos (umbral común =", UMBRAL, ")\n")
display(df_algos)

# Curvas ROC en una superpuestas
plt.figure(figsize=(7,5))
for nombre, fpr, tpr, auc_val in roc_curves:
    plt.plot(fpr, tpr, label=f"{nombre} (AUC={auc_val:.2f})")
plt.plot([0,1],[0,1],'--', color='gray')
plt.xlabel("Tasa de FP")
plt.ylabel("Tasa de VN")
plt.title("Curvas ROC - Comparativa de algoritmos")
plt.legend()
plt.tight_layout()
plt.show()

"""**Con la teoría y sabiendo el dataset y el objetivo del trabajo se podría saltar este paso, pero siempre es útil comprobar en la práctica que la regresión logística es el algoritmo que mejor se ajusta a esta situación.**

# 8. Evaluacion del modelo seleccionado con umbral 0.3 (LR)
"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score, roc_curve
print("Resultados en el conjunto de validacion")

y_prob = modelo.predict_proba(X_val)[:, 1]
umbral = 0.3
y_pred = (y_prob >= umbral).astype(int)

#Metricas
acc = accuracy_score(y_val, y_pred)
auc = roc_auc_score(y_val, y_prob)

print(f"Accuracy (umbral={umbral}): {acc:.4f}")
print(f"AUC: {auc:.4f}")
print("\nReporte de clasificación:")
print(classification_report(y_val, y_pred, digits=3))

# Subplot para ROC y matriz de confusion
fig, axes = plt.subplots(1, 2, figsize=(10, 4))

# Matriz de confusión
mc = confusion_matrix(y_val, y_pred)
sns.heatmap(mc, annot=True, fmt="d", cmap="Blues", cbar=False, ax=axes[0])
axes[0].set_title(f"Matriz de confusión (umbral={umbral})")
axes[0].set_xlabel("Predicho")
axes[0].set_ylabel("Real")

#Curva ROC
fpr, tpr, _ = roc_curve(y_val, y_prob)
axes[1].plot(fpr, tpr, label=f"AUC = {auc:.2f}")
axes[1].plot([0,1],[0,1],'--', color='gray')
axes[1].set_xlabel("Tasa FP")
axes[1].set_ylabel("Tasa VP")
axes[1].set_title("Curva ROC")
axes[1].legend()

plt.tight_layout()
plt.show()

"""# Conclusion umbral 0.3
Los resultados obtenidos muestran que, al ajustar el umbral de decisión a 0.3, el modelo de regresión logística logra una sensibilidad (recall) del 92% en la detección de pacientes con riesgo de enfermedad cardiovascular a 10 años. Este desempeño implica que la gran mayoría de los individuos en riesgo son correctamente identificados, lo cual es fundamental en el ámbito de la medicina preventiva. A cambio, el modelo presenta una disminución en la precisión (20%) y en la exactitud global (42%), generando un número elevado de falsos positivos. No obstante, en un contexto clínico este trade-off resulta aceptable, ya que es preferible derivar a pacientes aparentemente sanos a pruebas adicionales que dejar sin identificar a personas que realmente presentan riesgo. En consecuencia, el modelo, aun con limitaciones, constituye una herramienta útil de cribado preliminar, capaz de priorizar la identificación temprana de casos de alto riesgo.

# Comparativa de resultados
Resultados sin balancear datos
Resultados con datos balanceados pero umbral 0.5
Resultados balanceados y umbral 0.3
"""

"""Toda esta parte es extra, solo la uso para comparar como se ven los resultados
de los modelos si se le hacen ajustes sobre el umbral y el balanceo de clases"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

"""Esta funcion permite ajustar el umbral para explorar como cambian los
resultados"""

def evaluar_modelo(modelo, X, y, umbral=0.3):
    y_prob = modelo.predict_proba(X)[:, 1]
    y_pred = (y_prob >= umbral).astype(int)
    return {
        "Accuracy": accuracy_score(y, y_pred),
        "Precision": precision_score(y, y_pred, zero_division=0),
        "Recall": recall_score(y, y_pred, zero_division=0),
        "F1-score": f1_score(y, y_pred, zero_division=0),
        "AUC": roc_auc_score(y, y_prob),
        "CM": confusion_matrix(y, y_pred),
    }

# Modelos
modelo_original = LogisticRegression(max_iter=1000, random_state=42)
modelo_original.fit(X_train, y_train)
res_original = evaluar_modelo(modelo_original, X_val, y_val, umbral=0.5)

modelo_bal = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)
modelo_bal.fit(X_train, y_train)
res_bal = evaluar_modelo(modelo_bal, X_val, y_val, umbral=0.5)

res_bal_umbral = evaluar_modelo(modelo_bal, X_val, y_val, umbral=0.3)

# Comparativa de los resultados del reporte de clasificacion
df_comp = pd.DataFrame(
    [ {k:v for k,v in r.items() if k!="CM"} for r in [res_original, res_bal, res_bal_umbral] ],
    index=["Desbalanceado (0.5)", "Balanceado (0.5)", "Balanceado (0.3)"]
)
print("Comparativa de métricas:\n")
display(df_comp)

# Matriz de confusion x3
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
titulos = ["Original (0.5)", "Balanceado (0.5)", "Balanceado (0.3)"]
for ax, res, titulo in zip(axes, [res_original, res_bal, res_bal_umbral], titulos):
    sns.heatmap(res["CM"], annot=True, fmt="d", cmap="Blues", cbar=False, ax=ax)
    ax.set_title(titulo)
    ax.set_xlabel("Predicho")
    ax.set_ylabel("Real")
plt.tight_layout()
plt.show()

"""###Comparativa
**Izquierda**: Sin balanceo de clases y umbral predeterminado se consigue unicamente detectar 8 casos de pacientes en riesgo. Es excelente para detectar pacientes sanos porque el grupo en riesgo esta infrarepresentado en los datos originales.

**Centro**: Balanceo de clases con umbral predeterminado, ahora el grupo en riesgo ya no esta infrarepresentado pero el umbral impide que mas pacientes cumplas con la condicion de riesgo. Obtenemos mejores resultados pero seguimos sin capturar a 53 personas en riesgo.

**Derecha**: Clases balanceadas y umbral reducido que permite identificar a 119 pacientes en riesgo y tan solo quedan 10 por fuera. Ademas se puede ver que al bajar de 0.5 a 0.3 el numero de VN y FP se invirtieron de posicion.
"""